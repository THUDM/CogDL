{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "write-your-first-gcn",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHoQThpObPSq"
      },
      "source": [
        "\n",
        "\n",
        "**CogDL Notebook**\n",
        "created by CogDL Team\n",
        "[cogdlteam@gmail.com]\n",
        "\n",
        "This notebook shows how to write your first GCN model. \n",
        "\n",
        "CogDL Link: https://github.com/THUDM/CogDL\n",
        "\n",
        "Colab Link: https://colab.research.google.com/drive/1V47IIanXxDxi0Qsd6feOvvyYuqXcFP6P?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmSMqUEHCn7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iR3XYifiote"
      },
      "source": [
        "**第一部分：手动模拟GCN的计算和训练过程。**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtC_P6jTh9ta"
      },
      "source": [
        "1. 根据初始的邻接矩阵A得到正则化后的邻接矩阵normA。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7XKgoLPHLvv",
        "outputId": "39db09be-2b04-4b4e-ee94-c235485b1e20"
      },
      "source": [
        "A = torch.tensor([[0, 1, 1, 1], [1, 0, 1, 0], [1, 1, 0, 1], [1, 0, 1, 0]])\n",
        "A = A + torch.eye(4)\n",
        "print(\"A=\", A)\n",
        "# 计算度数矩阵D，并对A进行正则化得到normA\n",
        "D = torch.diag(A.sum(1))\n",
        "D_hat = torch.diag(1.0 / torch.sqrt(A.sum(1)))\n",
        "normA = torch.mm(torch.mm(D_hat, A), D_hat)\n",
        "print(\"normA=\", normA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A= tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "normA= tensor([[0.2500, 0.2887, 0.2500, 0.2887],\n",
            "        [0.2887, 0.3333, 0.2887, 0.0000],\n",
            "        [0.2500, 0.2887, 0.2500, 0.2887],\n",
            "        [0.2887, 0.0000, 0.2887, 0.3333]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-sjQ5IuiLOm"
      },
      "source": [
        "2. 根据初始特征X，模型参数W1，邻接矩阵normA来计算第一层的输出H1。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTrKFEnOHqKe",
        "outputId": "35b3dfb0-1bc8-47ce-fc4d-595596ac834e"
      },
      "source": [
        "H0 = X = torch.FloatTensor([[1,0], [0,1], [1,0], [1,1]])\n",
        "W1 = torch.tensor([[1, -0.5], [0.5, 1]], requires_grad=True)\n",
        "# 通过normA/H0/W1计算得到H1\n",
        "H1 = F.relu(torch.mm(normA, torch.mm(H0, W1)))\n",
        "print(H1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0774, 0.1830],\n",
            "        [0.7440, 0.0447],\n",
            "        [1.0774, 0.1830],\n",
            "        [1.0774, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgepM9zIiXf2"
      },
      "source": [
        "3. 计算第二层的输出H2和最后的输出Z。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgBcMPBjKZSN",
        "outputId": "883733f7-2bcd-4a23-c7ae-ca60162bfe72"
      },
      "source": [
        "W2 = torch.tensor([[0.5, -0.5], [1, 0.5]], requires_grad=True)\n",
        "# 通过normA/H1/W2计算得到H2和Z\n",
        "H2 = torch.mm(normA, torch.mm(H1, W2))\n",
        "print(\"H2=\", H2)\n",
        "Z = F.softmax(H2, dim=-1)\n",
        "print(\"Z=\", Z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "H2= tensor([[ 0.6366, -0.4800],\n",
            "        [ 0.5556, -0.3747],\n",
            "        [ 0.6366, -0.4800],\n",
            "        [ 0.5962, -0.4377]], grad_fn=<MmBackward>)\n",
            "Z= tensor([[0.7534, 0.2466],\n",
            "        [0.7171, 0.2829],\n",
            "        [0.7534, 0.2466],\n",
            "        [0.7377, 0.2623]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7cXQ_Lqh6D5"
      },
      "source": [
        "4. 计算损失函数loss。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvMmgve_MIiQ",
        "outputId": "1ca06b90-a2bb-446d-d7d0-28a4c24232e3"
      },
      "source": [
        "Y = torch.LongTensor([0, 1, 0, 0])\n",
        "# 根据输出Z和标签Y来计算最后的loss\n",
        "loss = F.nll_loss(Z.log(), Y)\n",
        "print(loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5333564281463623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMW8FEFghuzk"
      },
      "source": [
        "5. 通过loss进行反向传播。可以看到模型参数W1/W2的梯度值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAgWX9NwNK0_",
        "outputId": "1f518dd6-9ce7-428a-d4cb-84ab6ddaf41c"
      },
      "source": [
        "loss.backward(retain_graph=True)\n",
        "print(W1)\n",
        "print(W1.grad)\n",
        "print(W2)\n",
        "print(W2.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0000, -0.5000],\n",
            "        [ 0.5000,  1.0000]], requires_grad=True)\n",
            "tensor([[-0.0352,  0.0085],\n",
            "        [-0.0088,  0.0052]])\n",
            "tensor([[ 0.5000, -0.5000],\n",
            "        [ 1.0000,  0.5000]], requires_grad=True)\n",
            "tensor([[-0.0396,  0.0396],\n",
            "        [ 0.0018, -0.0018]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PmciUGbjBwQ"
      },
      "source": [
        "**第二部分：使用你实现的GCN模型来运行cora数据集**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbIKi_cqhoZw"
      },
      "source": [
        "1. 通过pip install来安装cogdl。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBGhvG4U9JBm",
        "outputId": "2463ca5f-d7a2-411f-8f46-cc79eeec3fae"
      },
      "source": [
        "!pip install cogdl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cogdl\n",
            "  Downloading cogdl-0.4.0-py3-none-any.whl (324 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 133 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 143 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 153 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 163 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 174 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 204 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 215 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 225 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 235 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 245 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 256 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 266 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 276 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 286 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 296 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 307 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 317 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 324 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from cogdl) (1.9.0+cu102)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from cogdl) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from cogdl) (0.51.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from cogdl) (3.2.2)\n",
            "Requirement already satisfied: gensim<4.0 in /usr/local/lib/python3.7/dist-packages (from cogdl) (3.6.0)\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.1-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting grave\n",
            "  Downloading grave-0.0.3-py3-none-any.whl (15 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.1 MB/s \n",
            "\u001b[?25hCollecting optuna==2.4.0\n",
            "  Downloading optuna-2.4.0-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 79.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from cogdl) (2.5.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from cogdl) (0.8.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cogdl) (1.4.1)\n",
            "Collecting flake8\n",
            "  Downloading flake8-3.9.2-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 79.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from cogdl) (4.41.1)\n",
            "Collecting pre-commit\n",
            "  Downloading pre_commit-2.14.0-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 60.4 MB/s \n",
            "\u001b[?25hCollecting texttable\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cogdl) (1.19.5)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.6.5-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 71.2 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.6.0\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.4.0->cogdl) (21.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.4.0->cogdl) (1.4.20)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.8.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from optuna==2.4.0->cogdl) (1.0.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0->cogdl) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0->cogdl) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna==2.4.0->cogdl) (2.4.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.4.0->cogdl) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna==2.4.0->cogdl) (4.6.1)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==2.4.0->cogdl) (2.8.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 80.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.4.0->cogdl) (3.13)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.4.0->cogdl) (2.1.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.1.2-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 80.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.4.0->cogdl) (3.7.4.3)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.4.0->cogdl) (21.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.4.0->cogdl) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna==2.4.0->cogdl) (3.5.0)\n",
            "Collecting pycodestyle<2.8.0,>=2.7.0\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 796 kB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting pyflakes<2.4.0,>=2.3.0\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==2.4.0->cogdl) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cogdl) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cogdl) (1.3.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->cogdl) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->cogdl) (57.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->cogdl) (0.34.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb->cogdl) (1.24.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb->cogdl) (1.1.5)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb->cogdl) (2.23.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb->cogdl) (2018.9)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pre-commit->cogdl) (0.10.2)\n",
            "Collecting PyYAML>=3.12\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting identify>=1.0.0\n",
            "  Downloading identify-2.2.13-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting cfgv>=2.0.0\n",
            "  Downloading cfgv-3.3.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting nodeenv>=0.11.1\n",
            "  Downloading nodeenv-1.6.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting virtualenv>=20.0.8\n",
            "  Downloading virtualenv-20.7.0-py2.py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 25.6 MB/s \n",
            "\u001b[?25hCollecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.2-py2.py3-none-any.whl (338 kB)\n",
            "\u001b[K     |████████████████████████████████| 338 kB 65.8 MB/s \n",
            "\u001b[?25hCollecting backports.entry-points-selectable>=1.0.4\n",
            "  Downloading backports.entry_points_selectable-1.1.0-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.2.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit->cogdl) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb->cogdl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb->cogdl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb->cogdl) (2021.5.30)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->cogdl) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->cogdl) (7.1.2)\n",
            "Building wheels for collected packages: pyperclip, emoji, littleutils\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=535bc9d9078e0ad20becc66c933247d36be84b06a93333c23bc990f6498e496f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=b1b5679d471c1aa61c7c2f3ffcc52fee5526b73477dde522487809df26222e5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/61/e7/2fc1ac8f306848fc66c6c013ab511f0a39ef4b1825b11363b2\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=be77aeac0fecfa3750801f821c1689a4601bd6a9c868a73ff4e69b7c2eba93f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built pyperclip emoji littleutils\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, PyYAML, python-editor, platformdirs, Mako, littleutils, distlib, cmd2, backports.entry-points-selectable, virtualenv, tokenizers, sacremoses, pyflakes, pycodestyle, outdated, nodeenv, mccabe, identify, huggingface-hub, colorlog, cmaes, cliff, cfgv, alembic, transformers, texttable, sentencepiece, pre-commit, optuna, ogb, grave, flake8, emoji, cogdl\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.1.4 PyYAML-5.4.1 alembic-1.6.5 backports.entry-points-selectable-1.1.0 cfgv-3.3.0 cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.2 cogdl-0.4.0 colorama-0.4.4 colorlog-5.0.1 distlib-0.3.2 emoji-1.4.2 flake8-3.9.2 grave-0.0.3 huggingface-hub-0.0.12 identify-2.2.13 littleutils-0.2.2 mccabe-0.6.1 nodeenv-1.6.0 ogb-1.3.1 optuna-2.4.0 outdated-0.2.1 pbr-5.6.0 platformdirs-2.2.0 pre-commit-2.14.0 pycodestyle-2.7.0 pyflakes-2.3.1 pyperclip-1.8.2 python-editor-1.0.4 sacremoses-0.0.45 sentencepiece-0.1.96 stevedore-3.3.0 texttable-1.6.4 tokenizers-0.10.3 transformers-4.9.1 virtualenv-20.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAPbTveYjONP"
      },
      "source": [
        "2. 从cogdl中加载cora数据集（x表示特征，y表示标签，mask表示训练/验证/测试集的划分）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61aEVO2ySq27",
        "outputId": "6da332b4-d5aa-4baf-d95e-4dd5b4b291da"
      },
      "source": [
        "from cogdl.datasets import build_dataset_from_name\n",
        "\n",
        "dataset = build_dataset_from_name(\"cora\")\n",
        "data = dataset[0]\n",
        "print(data)\n",
        "n = data.x.shape[0]\n",
        "edge_index = torch.stack(data.edge_index)\n",
        "A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (n, n)).to_dense()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Failed to load C version of sampling, use python version instead.\n",
            "Downloading https://cloud.tsinghua.edu.cn/d/6808093f7f8042bfa1f0/files/?p=%2Fcora.zip&dl=1\n",
            "unpacking cora.zip\n",
            "Processing...\n",
            "Done!\n",
            "Graph(x=[2708, 1433], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_index=[2, 10184])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcRyHUuIjhfT"
      },
      "source": [
        "3. 使用你实现的GCN模型进行训练（在GCN模型的forward中填入你在第一部分中写的代码）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr5lZC2eWCYm",
        "outputId": "c42b59ef-9637-4bac-e1f1-79a3f9f6228a"
      },
      "source": [
        "import math\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    y_true = y_true.squeeze().long()\n",
        "    preds = y_pred.max(1)[1].type_as(y_true)\n",
        "    correct = preds.eq(y_true).double()\n",
        "    correct = correct.sum().item()\n",
        "    return correct / len(y_true)\n",
        "\n",
        "class GCN(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_feats,\n",
        "        hidden_size,\n",
        "        out_feats,\n",
        "    ):\n",
        "        super(GCN, self).__init__()\n",
        "        self.out_feats = out_feats\n",
        "        self.W1 = nn.Parameter(torch.FloatTensor(in_feats, hidden_size))\n",
        "        self.W2 = nn.Parameter(torch.FloatTensor(hidden_size, out_feats))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.out_feats)\n",
        "        torch.nn.init.uniform_(self.W1, -stdv, stdv)\n",
        "        torch.nn.init.uniform_(self.W2, -stdv, stdv)\n",
        "\n",
        "    def forward(self, A, X):\n",
        "        n = X.shape[0]\n",
        "        A = A + torch.eye(n, device=X.device)\n",
        "        # 依次计算normA/H1/H2，然后返回H2。注意：此处不需要计算Z，因为通常直接根据H2和Y来计算loss。\n",
        "        # 注意使用self.W1/W2来调用模型参数。\n",
        "        D_hat = torch.diag(1.0 / torch.sqrt(A.sum(1)))\n",
        "        normA = torch.mm(torch.mm(D_hat, A), D_hat)\n",
        "        H1 = F.relu(torch.mm(normA, torch.mm(X, self.W1)))\n",
        "        H2 = torch.mm(normA, torch.mm(H1, self.W2))\n",
        "\n",
        "        return H2\n",
        "\n",
        "\n",
        "hidden_size = 64\n",
        "model = GCN(data.x.shape[1], hidden_size, data.y.max() + 1)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "    A = A.to(device)\n",
        "    data.apply(lambda x: x.to(device))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "epoch_iter = tqdm(range(100), position=0, leave=True)\n",
        "best_model = None\n",
        "best_loss = 1e8\n",
        "for epoch in epoch_iter:\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(A, data.x)\n",
        "    loss = F.cross_entropy(logits[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss = loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(A, data.x)\n",
        "        val_loss = F.cross_entropy(logits[data.val_mask], data.y[data.val_mask]).item()\n",
        "        val_acc = accuracy(logits[data.val_mask], data.y[data.val_mask])\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "    epoch_iter.set_description(f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = best_model(A, data.x)\n",
        "    val_acc = accuracy(logits[data.val_mask], data.y[data.val_mask])\n",
        "    test_acc = accuracy(logits[data.test_mask], data.y[data.test_mask])\n",
        "print(\"Val Acc\", val_acc)\n",
        "print(\"Test Acc\", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 099, Train Loss: 0.0112, Val Loss: 0.7426, Val Acc: 0.7820: 100%|██████████| 100/100 [00:03<00:00, 26.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val Acc 0.786\n",
            "Test Acc 0.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG_u8LIdkQh_"
      },
      "source": [
        "4. 调用cogdl的GCN模型来运行cora数据集，观察两者的区别（包括Acc和训练时间）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOdtm6g_HgRx",
        "outputId": "a4070f98-de47-49c0-cbfd-82b093553ad4"
      },
      "source": [
        "from cogdl import experiment\n",
        "\n",
        "experiment(task=\"node_classification\", dataset=\"cora\", model=\"gcn\", max_epoch=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 008, Train: 0.9571, Val: 0.7400, ValLoss: 1.8320:   3%|▎         | 3/100 [00:00<00:03, 28.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Namespace(activation='relu', checkpoint=None, cpu=False, dataset='cora', device_id=[0], dropout=0.5, fast_spmm=False, hidden_size=64, inference=False, lr=0.01, max_epoch=100, missing_rate=0, model='gcn', norm=None, num_classes=None, num_features=None, num_layers=2, patience=100, residual=False, save_dir='.', save_model=None, seed=1, task='node_classification', trainer=None, use_best_config=False, weight_decay=0.0005)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 099, Train: 1.0000, Val: 0.7880, ValLoss: 0.7775: 100%|██████████| 100/100 [00:00<00:00, 112.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Valid accurracy =  0.7880\n",
            "Test accuracy = 0.8090\n",
            "| Variant         | Acc           | ValAcc        |\n",
            "|-----------------|---------------|---------------|\n",
            "| ('cora', 'gcn') | 0.8090±0.0000 | 0.7880±0.0000 |\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list, {('cora', 'gcn'): [{'Acc': 0.809, 'ValAcc': 0.788}]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_MIqEgvk9T1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}